# Azure AI Evaluator

A standalone component for evaluating generative AI outputs using Azure AI evaluation metrics.

## Overview

This component allows you to evaluate the quality of generative AI outputs using various metrics provided by the Azure AI Evaluation SDK. It reads a JSONL file where each line contains a query, generated text, and reference text, then applies selected evaluators to measure different aspects of the generated content.

## Features

- Supports multiple evaluation metrics:
  - **Quality metrics**: Relevance, Coherence, Fluency
  - **Textual similarity**: BLEU, ROUGE, F1, GLEU, METEOR
  - **Other metrics**: Groundedness, Similarity
- Generates comprehensive evaluation reports in both JSON and Markdown formats
- Runs as a standalone script with command-line interface
- Configurable for different evaluation needs

## Requirements

- Python 3.9 or later
- `azure-ai-evaluation` package
- Azure OpenAI endpoint and API key (for AI-assisted evaluators)

## Installation

Ensure you have the required package installed:

```bash
pip install azure-ai-evaluation
```

Or simply install all project dependencies:

```bash
pip install -r requirements.txt
```

## Usage

### Basic Usage

```bash
python src/azure_evaluator.py --input data/sample_dataset.json
```

This will evaluate the dataset using default metrics (relevance, coherence, fluency, bleu, rouge) and save the results to the `output` directory.

The evaluator supports both JSON and JSONL file formats:
- JSON: An array of objects with `query`, `generated_text`, and `reference_text` fields
- JSONL: Each line is a JSON object with the same required fields

### Specifying Metrics

```bash
python src/azure_evaluator.py --input data/sample_dataset.json --metrics bleu,rouge,meteor
```

### Using AI-Assisted Evaluators

For metrics like relevance, coherence, and fluency, you need to provide Azure OpenAI credentials:

```bash
python src/azure_evaluator.py --input data/sample_dataset.json --metrics relevance,coherence,fluency \
  --model-endpoint "https://your-endpoint.openai.azure.com/" \
  --model-api-key "your-api-key" \
  --model-deployment "your-deployment-name"
```

### Saving Results to a Specific Location

```bash
python src/azure_evaluator.py --input data/sample_dataset.json --output results/my_evaluation.json
```

### Verbose Output

```bash
python src/azure_evaluator.py --input data/sample_dataset.json --verbose
```

## Input Format

The input JSONL file should have each line formatted as a JSON object with the following fields:

```json
{
  "query": "The user query or input",
  "generated_text": "The text generated by the AI model",
  "reference_text": "The reference or ground truth text"
}
```

## Output

The component generates two output files:

1. **JSON file**: Contains detailed evaluation results including per-example scores
2. **Markdown file**: Contains a summary of the evaluation results in a readable format

## Example Output (Markdown)

```markdown
# Azure AI Evaluation Results

*Generated on: 2025-07-03 15:30:45*

## Summary

| Metric | Score | Binary Result |
|--------|-------|---------------|
| bleu_score | 0.7532 | N/A |
| rouge_score | 0.6821 | N/A |
| coherence | 0.9123 | Pass |
| fluency | 0.8975 | Pass |

## Detailed Results

Detailed results are available in the JSON file.
```
